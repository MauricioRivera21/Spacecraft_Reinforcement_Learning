# -*- coding: utf-8 -*-
"""Proyecto_DeepQRL (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zeglM-BPlF0XO0ia9R_2QJN_Qd-pUckw
"""

import gym 
import matplotlib.pyplot as plt
import tensorflow as tf
import random
import numpy as np
from keras import layers 
from keras import Sequential
from collections import deque
import pickle as cPickle

#!pip install gym[box2d]
env = gym.make('LunarLander-v2')
np.random.seed(21)
env.reset()

class DQN:
    def __init__(self, env, batch_size = 32, lr = 1e-4, gamma = 0.99, epsilon = 0.1, epsilon_decay = 0.995,  epsilon_min=0.01, early_stopping_reward = 150):

        self.env = env
        self.lr = lr
        self.model = self.create_model()
        self.gamma = gamma
        self.global_reward = []
        self.global_avg_reward = []
        self.epsilon = epsilon
        self.epsilon_min = epsilon_min
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min
        self.agent_buffer = deque(maxlen=500000)
        self.early_stopping_reward = early_stopping_reward
        self.batch_size = batch_size #Se preferencia que sean multiples de 2 

    def get_action_greedy(self, state):
        """
        Utiliza el algoritmo e-greedy para el sampleo de la acción según
        una política. Dos son los posibles outputs: acción aleatoria o predicha
        por el modelo entrenado hasta ese punto
        """
        do_exploration = np.random.uniform(0, 1) < self.epsilon
        if do_exploration:
            a = np.random.randint(0, self.env.action_space.n)
        else:
            state = np.expand_dims(state, axis=0)
            a = np.argmax(self.model.predict(state))
        return a 

    def save_memory(self,state,action,reward,next_state,done):
        """
        Guardar información temporal del entrenamiento en una memoria 
        apilable para samplear de este elemento en el entrenamiento de la 
        siguiente iteración 
        """    
        self.agent_buffer.append((state, action, reward, next_state, done))

    def create_model(self):
        """
        Crea una red neuronal de 2 capas para 8 entradas (estados) y 4 salidas (acciones)
        """
        model = Sequential()
        model.add(layers.Dense(128, activation='relu', input_shape=[self.env.observation_space.shape[0]]))
        model.add(layers.Dense(64, activation='relu'))
        model.add(layers.Dense(self.env.action_space.n))

        optimizer = tf.keras.optimizers.Adam(learning_rate= self.lr) 
        model.compile(loss='mse',
                        optimizer=optimizer,
                        metrics=['mae', 'mse'])

        print(model.summary())
        return model 

    def update_model(self):
        """
        Esta función entrena el modelo por una iteración utilizando el modelo de
        Q-learning luego de samplear un conjunto de acción,estado de la memoria buffer.
        El batch_size, determina que tan grande es el vector de entrenamiento 
        """
         # Si la memoria es menor al batch el programa termina 
        if len(self.agent_buffer) < self.batch_size:
            return
        # Termina antes si el valor promedio de los últimos 10 rewards
        # es mayor a 180
        if np.mean(self.global_reward[-10:]) > self.early_stopping_reward:
            return

        env_random_sample = random.sample(self.agent_buffer,self.batch_size) # Obtiene una matrix (batch_size,5) 
        states, actions, rewards, next_states, done_list = self.unpack_env_info(env_random_sample) # Separa los datos # Chequear         
        Q = rewards + self.gamma * (np.amax(self.model.predict_on_batch(next_states), axis=1)) * (1 - done_list) 
        output = self.model.predict_on_batch(states) 
        output[np.arange(self.batch_size), [actions]] = Q 

        # Entrena el modelo
        self.model.fit(states, output, epochs=1, verbose=0) 

    def unpack_env_info(self,random_sample):
        """
        Para el entrenamiento se debe formar un vector de [batch_size,1] para estado, otro para acción ...
        Este método se encarga de recuperar esa info para actualizar el Q
        """
        states = np.array([i[0] for i in random_sample])
        actions = np.array([i[1] for i in random_sample])
        rewards = np.array([i[2] for i in random_sample])
        next_states = np.array([i[3] for i in random_sample])
        done_list = np.array([i[4] for i in random_sample])

        states = np.squeeze(states)  
        next_states = np.squeeze(next_states)
        
        return states,actions,rewards,next_states,done_list


    def train_model(self, num_episodes = 100, num_steps = 1000):

        for i_episode in range(1, num_episodes+1):
            # Initializar un episodio
            max_avg_reward = -np.inf
            s_t = env.reset()            # Estado inicial del episodio
            total_reward = 0                 # Reward total del episodio
            done = False                     # Condición de salida             
            print("Episodio ", i_episode)

            for _ in range(num_steps):       # Lopea el episodio 
                a_t = self.get_action_greedy(s_t) # Genera una acción[k] para el estado inicial 
                s_t1, reward, done ,info = env.step(a_t) # Genera estado[k+1], reward y condicion para la acción[k] 
                self.save_memory(s_t,a_t,reward,s_t1,done) # Guarda info estado[k],acción[k],reward,estado[k+1]
                total_reward += reward # Acumula reward
                s_t = s_t1 #Actualiza estado
                self.update_model() #Actualiza modelo
                
                if done: # Termino el episodio
                    break 

            # Almacenar recompensa final
            self.global_reward.append(total_reward)
          
            # Actualiza el epsilon hasta el valor mínimo
            if self.epsilon > self.epsilon_min:
                self.epsilon*=self.epsilon_decay

            # Mostrar en qué episodio se está
            if len(self.global_reward) > 5:
                avg_reward = np.mean(self.global_reward[-5:])
                if avg_reward > max_avg_reward:
                    max_avg_reward = avg_reward
            
            self.global_avg_reward.append(max_avg_reward)

            if i_episode % 5 == 0:
                output_str = "\r lr:{} | gamma:{} | e:{} | Episodio {}/{} | Max Recompensa Promedio: {}".format(self.lr,self.gamma,self.epsilon,i_episode, num_episodes, max_avg_reward)
                print(output_str)
                file = open("C:/Users/17802/Dropbox/PC/Downloads/chunck/resultados.txt","a")
                file.write(output_str)
                file.close()
                model_name = "modelo_lr{}_epi.h5".format(self.lr,i_episode)
                cPickle.dump(self.model, open('modelo_save', "wb"))
                self.model.save(model_name)

    def plot_results(self):
        
        plt.title("Rendimiento de aprendizaje")
        plt.xlabel("Episodios")
        plt.ylabel("Recompensa")
        plt.plot(self.global_reward)
        plt.plot(self.global_avg_reward,'r')
        plt.savefig("C:/Users/17802/Dropbox/PC/Downloads/chunck/results.png")

def eval_lr(env, num_episodes = 2000):
    lr_list = [1e-4, 5e-4, 1e-3, 5e-3, 2e-2]
    results = []
    for lr in lr_list:
        agente = DQN(env, lr = lr)
        agente.train_model(num_episodes= num_episodes)
        results.append(agente.global_avg_reward[-1])

    with open("/content/drive/MyDrive/Ciclo 8/Robotica Avanzada/Proyecto/eval_lr.txt", "w") as f:
        for lr,r in zip(lr_list,results):
            f.write(f'Learning rate: {lr}, reward: {r}')
            f.write("\n")

def eval_gamma(env, num_episodes = 2000):
    gamma_list = [0.5,0.75,0.9,0.99,0.999]
    results = []
    for gamma in gamma_list:
        agente = DQN(env, gamma = gamma)
        agente.train_model(num_episodes= num_episodes)
        results.append(agente.global_avg_reward[-1])

    with open("/content/drive/MyDrive/Ciclo 8/Robotica Avanzada/Proyecto/eval_gamma.txt", "w") as f:
        for gamma,r in zip(gamma_list,results):
            f.write(f'Gamma: {gamma}, reward: {r}')
            f.write("\n")

def eval_epsilon(env, num_episodes = 2000):
    epsilon_list = [0.1, 0.25, 0.3, 0.4, 0.5]
    results = []
    for epsilon in epsilon_list:
        agente = DQN(env, epsilon = epsilon)
        agente.train_model(num_episodes = num_episodes)
        results.append(agente.global_avg_reward[-1])
    
    with open("/content/drive/MyDrive/Ciclo 8/Robotica Avanzada/Proyecto/eval_epsilon.txt", "w") as f:
        for epsilon,r in zip(epsilon_list,results):
            f.write(f'Epsilon: {epsilon}, reward: {r}')
            f.write("\n")

def test_already_trained_model(trained_model):
    rewards_list = []
    num_test_episode = 3
    env = gym.make("LunarLander-v2")
    print("Starting Testing of the trained model...")

    step_count = 1000

    for test_episode in range(num_test_episode):
        current_state = env.reset()
        num_observation_space = env.observation_space.shape[0]
        current_state = np.reshape(current_state, [1, num_observation_space])
        reward_for_episode = 0
        for step in range(step_count):
            #env.render()
            selected_action = np.argmax(trained_model.predict(current_state)[0])
            new_state, reward, done, info = env.step(selected_action)
            new_state = np.reshape(new_state, [1, num_observation_space])
            current_state = new_state
            reward_for_episode += reward
            if done:
                break
        rewards_list.append(reward_for_episode)
        print(test_episode, "\t: Episode || Reward: ", reward_for_episode)

    return rewards_list


def main():
    #model = DQN(env, lr = 5e-4)
    #model.train_model()
    #model.plot_results()

    #eval_lr(env)
    #eval_epsilon(env)
    #eval_gamma(env)

    # Test the model


    with open('modelo_save', 'rb') as fp:
        trained_model = cPickle.load(fp)

    test_rewards = test_already_trained_model(trained_model)
    cPickle.dump(test_rewards, open('test_rewards.p', "wb"))
    test_rewards = cPickle.load(open("test_rewards.p", "rb"))

    #trained_model = load_model("trained_model.h5")
    
    
    
if __name__ == "__main__":
    main()
